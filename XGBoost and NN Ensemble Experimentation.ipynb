{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For compatibility across multiple platforms\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "# Load files using DictReader in Python\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from ultimate.mlp import MLP \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/numpy/lib/arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_filepath = \"pubg-finish-placement-prediction/train_V2_clean.csv\"\n",
    "\n",
    "#trainset_file = open(train_filepath,'rU')\n",
    "trainset = pd.read_csv(train_filepath,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4446965\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_two(moddedTrain):\n",
    "    moddedTrain['playersJoined'] = moddedTrain.groupby('matchId')['matchId'].transform('count')\n",
    "    moddedTrain['killsNorm'] = moddedTrain['kills']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['damageDealtNorm'] = moddedTrain['damageDealt']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['maxPlaceNorm'] = moddedTrain['maxPlace']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['matchDurationNorm'] = moddedTrain['matchDuration']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['healsandboosts'] = moddedTrain['heals'] + moddedTrain['boosts']\n",
    "    moddedTrain['totalDistance'] = moddedTrain['rideDistance'] + moddedTrain['walkDistance'] + moddedTrain['swimDistance']\n",
    "    moddedTrain['killsWithoutMoving'] = ((moddedTrain['kills'] > 0) & (moddedTrain['totalDistance'] == 0))\n",
    "    moddedTrain['headshot_rate'] = moddedTrain['headshotKills'] / moddedTrain['kills']\n",
    "    moddedTrain['headshot_rate'] = moddedTrain['headshot_rate'].fillna(0)\n",
    "    moddedTrain.drop(moddedTrain[moddedTrain['killsWithoutMoving'] == True].index, inplace=True)\n",
    "    moddedTrain.drop(moddedTrain[moddedTrain['roadKills'] > 8].index, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(trainset,is_train=True):\n",
    "    # When this function is used for the training data, load train_V2.csv :\n",
    "    if is_train: \n",
    "        print(\"processing train_V2.csv\")\n",
    "        #df = pd.read_csv(\"pubg-finish-placement-prediction/train_V2_clean.csv\",index_col=0)\n",
    "        df = trainset\n",
    "        # Only take the samples with matches that have more than 1 player \n",
    "        # there are matches with no players or just one player ( those samples could affect our model badly) \n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    \n",
    "    # When this function is used for the test data, load test_V2.csv :\n",
    "    else:\n",
    "        print(\"processing test_V2.csv\")\n",
    "        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "        \n",
    "    # Make a new feature indecating the total distance a player cut :\n",
    " \n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "\n",
    "    # Process the 'rankPoints' feature by replacing any value of (-1) to be (0) :\n",
    "    df['rankPoints'] = np.where(df['rankPoints'] <= 0 ,0 , df['rankPoints'])\n",
    "                           \n",
    "    \n",
    "\n",
    "    target = 'winPlacePerc'\n",
    "    # Get a list of the features to be used\n",
    "    features = list(df.columns)\n",
    "    \n",
    "    # Remove some features from the features list :\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchDuration\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    y = None\n",
    "    \n",
    "    # If we are processing the training data, process the target\n",
    "    # (group the data by the match and the group then take the mean of the target) \n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        # Remove the target from the features list :\n",
    "        features.remove(target)\n",
    "    \n",
    "    # Make new features indicating the mean of the features ( grouped by match and group ) :\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    \n",
    "    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n",
    "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
    "    # If we are processing the test data let df_out = 'matchId' and 'groupId' without grouping \n",
    "    else: df_out = df[['matchId','groupId']]\n",
    "    \n",
    "    # Merge agg and agg_rank (that we got before) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the max value of the features for each group ( grouped by match )\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the number of players in each group ( grouped by match )\n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "     \n",
    "    # Merge the group_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the mean value of each features for each match :\n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    \n",
    "    # Merge the new agg with df_out :\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    # Make new features indicating the number of groups in each match :\n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    \n",
    "    # Merge the match_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    # Drop matchId and groupId\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "    \n",
    "    y = y.tolist()\n",
    "    \n",
    "\n",
    "    return df_out,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing train_V2.csv\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n"
     ]
    }
   ],
   "source": [
    "feature_engineering_two(trainset)\n",
    "df_out,y = feature_engineering(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_out, y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# SET UP FOR MODEL ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the trainset features for tree one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = x_train\n",
    "y_one = y_train\n",
    "\n",
    "x_one_val = x_val\n",
    "y_one_val = y_val\n",
    "\n",
    "\n",
    "#set the dmatrix\n",
    "#trainset_one_dmatrix = xgb.DMatrix(x_one.values,label=y_train.values,feature_names=x_one.columns)\n",
    "#valset_one_dmatrix = xgb.DMatrix(x_one_val.values,label=y_one_val.values,feature_names=x_one_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_one_dmatrix = xgb.DMatrix(x_one.values,label=y_train,feature_names=x_one.columns)\n",
    "valset_one_dmatrix = xgb.DMatrix(x_one_val.values,label=y_one_val,feature_names=x_one_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assists_mean</th>\n",
       "      <th>boosts_mean</th>\n",
       "      <th>damageDealt_mean</th>\n",
       "      <th>DBNOs_mean</th>\n",
       "      <th>headshotKills_mean</th>\n",
       "      <th>heals_mean</th>\n",
       "      <th>killPlace_mean</th>\n",
       "      <th>killPoints_mean</th>\n",
       "      <th>kills_mean</th>\n",
       "      <th>killStreaks_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>rideDistance</th>\n",
       "      <th>roadKills</th>\n",
       "      <th>swimDistance</th>\n",
       "      <th>teamKills</th>\n",
       "      <th>vehicleDestroys</th>\n",
       "      <th>walkDistance</th>\n",
       "      <th>weaponsAcquired</th>\n",
       "      <th>winPoints</th>\n",
       "      <th>totalDistance</th>\n",
       "      <th>match_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>245.205000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1334.250000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1790.259341</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>1.617575</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>1452.990000</td>\n",
       "      <td>3.791209</td>\n",
       "      <td>1508.857143</td>\n",
       "      <td>3244.866915</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041049</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>204.793333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1389.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>41.771429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.151837</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1027.133776</td>\n",
       "      <td>3.673469</td>\n",
       "      <td>1508.500000</td>\n",
       "      <td>1072.057041</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>101.013333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1248.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>57.019175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316598</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>1212.426598</td>\n",
       "      <td>3.051546</td>\n",
       "      <td>1518.762887</td>\n",
       "      <td>1269.762371</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>69.852041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.222857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>936.041653</td>\n",
       "      <td>3.030612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1008.116551</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024780</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>285.380000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1300.600000</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1647.362366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1801.833548</td>\n",
       "      <td>4.204301</td>\n",
       "      <td>1495.451613</td>\n",
       "      <td>3449.195914</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         assists_mean  boosts_mean  damageDealt_mean  DBNOs_mean  \\\n",
       "240822            1.0     2.750000        245.205000         0.5   \n",
       "1041049           1.0     1.000000        204.793333         1.0   \n",
       "1677229           0.0     0.333333        101.013333         1.0   \n",
       "1378932           0.0     0.000000         50.000000         0.5   \n",
       "2024780           0.2     1.800000        285.380000         1.6   \n",
       "\n",
       "         headshotKills_mean  heals_mean  killPlace_mean  killPoints_mean  \\\n",
       "240822                 0.25        5.25            29.0      1334.250000   \n",
       "1041049                0.00        1.00            41.0      1389.666667   \n",
       "1677229                0.00        0.00            89.0      1248.666667   \n",
       "1378932                0.00        0.00            71.0         0.000000   \n",
       "2024780                0.20        7.20            21.2      1300.600000   \n",
       "\n",
       "         kills_mean  killStreaks_mean     ...      rideDistance  roadKills  \\\n",
       "240822          0.5          0.500000     ...       1790.259341   0.010989   \n",
       "1041049         1.0          0.666667     ...         41.771429   0.000000   \n",
       "1677229         0.0          0.000000     ...         57.019175   0.000000   \n",
       "1378932         0.5          0.500000     ...         69.852041   0.000000   \n",
       "2024780         2.2          1.000000     ...       1647.362366   0.000000   \n",
       "\n",
       "         swimDistance  teamKills  vehicleDestroys  walkDistance  \\\n",
       "240822       1.617575   0.010989         0.032967   1452.990000   \n",
       "1041049      3.151837   0.010204         0.000000   1027.133776   \n",
       "1677229      0.316598   0.030928         0.010309   1212.426598   \n",
       "1378932      2.222857   0.000000         0.000000    936.041653   \n",
       "2024780      0.000000   0.032258         0.000000   1801.833548   \n",
       "\n",
       "         weaponsAcquired    winPoints  totalDistance  match_size  \n",
       "240822          3.791209  1508.857143    3244.866915          91  \n",
       "1041049         3.673469  1508.500000    1072.057041          98  \n",
       "1677229         3.051546  1518.762887    1269.762371          97  \n",
       "1378932         3.030612     0.000000    1008.116551          98  \n",
       "2024780         4.204301  1495.451613    3449.195914          93  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure tree one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\" : 7,\n",
    "    \"eval_metric\" : [\"mae\"],\n",
    "    \"lambda\": 1.1,   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[0]\ttrain-mae:0.184906\tval-mae:0.184939\n",
      "[15:42:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[1]\ttrain-mae:0.134033\tval-mae:0.134024\n",
      "[15:43:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[2]\ttrain-mae:0.099873\tval-mae:0.099852\n",
      "[15:44:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[3]\ttrain-mae:0.077337\tval-mae:0.077323\n",
      "[15:45:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[4]\ttrain-mae:0.062542\tval-mae:0.062545\n",
      "[15:46:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[5]\ttrain-mae:0.052687\tval-mae:0.052702\n",
      "[15:47:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[6]\ttrain-mae:0.046311\tval-mae:0.046347\n",
      "[15:48:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[7]\ttrain-mae:0.042466\tval-mae:0.042521\n",
      "[15:49:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[8]\ttrain-mae:0.039752\tval-mae:0.039835\n",
      "[15:50:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[9]\ttrain-mae:0.038094\tval-mae:0.038196\n",
      "[15:50:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[10]\ttrain-mae:0.037007\tval-mae:0.037126\n",
      "[15:51:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[11]\ttrain-mae:0.036254\tval-mae:0.036388\n",
      "[15:52:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12]\ttrain-mae:0.03552\tval-mae:0.035671\n",
      "[15:53:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[13]\ttrain-mae:0.035031\tval-mae:0.035204\n",
      "[15:54:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[14]\ttrain-mae:0.034726\tval-mae:0.034904\n",
      "[15:55:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[15]\ttrain-mae:0.03448\tval-mae:0.034675\n",
      "[15:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[16]\ttrain-mae:0.034275\tval-mae:0.034484\n",
      "[15:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[17]\ttrain-mae:0.034104\tval-mae:0.034324\n",
      "[15:58:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[18]\ttrain-mae:0.033945\tval-mae:0.034177\n",
      "[15:59:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[19]\ttrain-mae:0.033775\tval-mae:0.034018\n",
      "[15:59:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[20]\ttrain-mae:0.033551\tval-mae:0.033805\n",
      "[16:00:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[21]\ttrain-mae:0.033378\tval-mae:0.033639\n",
      "[16:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[22]\ttrain-mae:0.033228\tval-mae:0.033501\n",
      "[16:02:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[23]\ttrain-mae:0.033122\tval-mae:0.033399\n",
      "[16:03:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[24]\ttrain-mae:0.032972\tval-mae:0.033258\n",
      "[16:04:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[25]\ttrain-mae:0.032802\tval-mae:0.033096\n",
      "[16:05:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[26]\ttrain-mae:0.032682\tval-mae:0.032986\n",
      "[16:06:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[27]\ttrain-mae:0.032593\tval-mae:0.032907\n",
      "[16:07:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[28]\ttrain-mae:0.032477\tval-mae:0.0328\n",
      "[16:08:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[29]\ttrain-mae:0.032436\tval-mae:0.032768\n"
     ]
    }
   ],
   "source": [
    "iters = 30\n",
    "\n",
    "tree_one = xgb.train(params, trainset_one_dmatrix, evals=[(trainset_one_dmatrix, \"train\"),(valset_one_dmatrix, 'val')], num_boost_round = iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb.plot_importance(tree_one,importance_type='cover')\n",
    "xgb.plot_importance(tree_one,importance_type='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP FOR MODEL TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the trainset features for tree two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_two = x_train\n",
    "y_two = y_train\n",
    "\n",
    "x_two_val = x_val\n",
    "y_two_val = y_val\n",
    "\n",
    "#scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -1.        , -0.98992342, ...,  0.920351  ,\n",
       "        -0.69493471,  0.95789474],\n",
       "       [-0.96825397, -0.95959596, -0.92319629, ..., -1.        ,\n",
       "        -0.6123272 ,  0.91578947],\n",
       "       [-1.        , -0.87878788, -0.95815599, ...,  0.93947277,\n",
       "        -0.37598127,  0.91578947],\n",
       "       ...,\n",
       "       [-1.        , -0.96969697, -0.98488513, ..., -1.        ,\n",
       "        -0.74718103,  0.87368421],\n",
       "       [-0.97619048, -1.        , -0.97965538, ...,  0.93687894,\n",
       "        -0.71874774,  0.87368421],\n",
       "       [-1.        , -0.90909091, -0.98606409, ...,  0.93673303,\n",
       "        -0.43835967,  0.89473684]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler.transform(x_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 136)               23256     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 136)               544       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               13700     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 167,407\n",
      "Trainable params: 167,135\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create NN_model\n",
    "NN_model = Sequential()\n",
    "NN_model.add(Dense(x_two.shape[1],  input_dim = x_two.shape[1], activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(BatchNormalization())\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(100, activation='relu'))\n",
    "NN_model.add(Dense(100, activation='relu'))\n",
    "NN_model.add(Dense(100, activation='relu'))\n",
    "NN_model.add(Dense(100, activation='relu'))\n",
    "NN_model.add(Dense(80, activation='relu'))\n",
    "NN_model.add(Dense(80, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output Layer\n",
    "NN_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297116 samples, validate on 324279 samples\n",
      "Epoch 1/5\n",
      "1297116/1297116 [==============================] - 51s 40us/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.0573 - val_mean_absolute_error: 0.0573\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.04533\n",
      "Epoch 2/5\n",
      "1297116/1297116 [==============================] - 37s 28us/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.0630 - val_mean_absolute_error: 0.0630\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04533\n",
      "Epoch 3/5\n",
      "1297116/1297116 [==============================] - 37s 28us/step - loss: 0.0489 - mean_absolute_error: 0.0489 - val_loss: 0.0673 - val_mean_absolute_error: 0.0673\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04533\n",
      "Epoch 4/5\n",
      "1297116/1297116 [==============================] - 36s 28us/step - loss: 0.0465 - mean_absolute_error: 0.0465 - val_loss: 0.0535 - val_mean_absolute_error: 0.0535\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04533\n",
      "Epoch 5/5\n",
      "1297116/1297116 [==============================] - 37s 28us/step - loss: 0.0449 - mean_absolute_error: 0.0449 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24ab57d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(x=x_two, y=y_two, batch_size=1000,\n",
    "             epochs=5, verbose=1, callbacks=callbacks_list,\n",
    "             validation_split=0.20, validation_data=None, shuffle=True,\n",
    "             class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "             steps_per_epoch=None, validation_steps=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSENBLE MODELS ON VALSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict - Update - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_predictions(models,labels,num_samples):\n",
    "    labels = labels[:num_samples]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        data = model['data']\n",
    "        predictor = model['predictor']\n",
    "\n",
    "        if model['type'] == \"tree\":\n",
    "            matrix = xgb.DMatrix(data,feature_names=data.columns)\n",
    "            prediction = predictor.predict(matrix)\n",
    "            predictions.append(prediction)\n",
    "            print(\"tree\")\n",
    "        if model['type'] == \"nn\":\n",
    "            prediction = predictor.predict(data)\n",
    "            predictions.append(prediction)\n",
    "            print(\"nn\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        \n",
    "    print(len(predictions[0]))\n",
    "    print(len(predictions[1]))\n",
    "\n",
    "    avg_predictions = [0.0]*len(predictions[0])\n",
    "    weight = 0\n",
    "    for j in range(len(predictions[0])):\n",
    "        if predictions[0][j] - predictions[1][j] <= .0001:\n",
    "            avg_predictions[j] = 1.0 * (predictions[0][j] + predictions[1][j]) / 2.0\n",
    "        else:\n",
    "            avg_predictions[j] = predictions[0][j]\n",
    "\n",
    "    print(len(avg_predictions))\n",
    "    return avg_predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn\n",
      "tree\n",
      "405349\n",
      "405349\n",
      "405349\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append({'data':x_one_val,\n",
    "                'predictor':NN_model,\n",
    "                'type':\"nn\"})\n",
    "\n",
    "models.append({'data':x_one_val,\n",
    "                'predictor':tree_one,\n",
    "                'type':\"tree\"})\n",
    "\n",
    "labels = y_val\n",
    "\n",
    "num_samples = 200000\n",
    "\n",
    "predictions = gen_predictions(models,labels,num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042158966795849594\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.mean_absolute_error(labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032768144979863266\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.mean_absolute_error(labels,tree_one.predict(valset_one_dmatrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
