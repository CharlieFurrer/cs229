{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSV files are essentially identical to CSV files except that TSV files use \"tabs (\\t)\" while CSV files use commas to store data in tabular structure. As a result, loading TSV files are slightly different from how we've been loading CSV files.\n",
    "##### Thanks to Clara Meister for providing this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For compatibility across multiple platforms\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import seaborn as sn\n",
    "\n",
    "# Load files using DictReader in Python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import sklearn\n",
    "import sklearn_pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                 Id            groupId            matchId  \\\n",
      "0           0  35913017459246474  21756414768994750  45321147693812369   \n",
      "1           1  67264846101073980  29358430787743646  49173965273764108   \n",
      "2           2   8637285204745842  29917998133566068   4786602953643182   \n",
      "3           3  19728345572649043  47622776820651809  68101503675608446   \n",
      "4           4  13894076435569324  62491847359224029  30901772270576102   \n",
      "\n",
      "   assists  boosts  damageDealt  DBNOs  headshotKills  heals      ...       \\\n",
      "0        0       0         0.00      0              0      0      ...        \n",
      "1        0       0        91.47      0              0      0      ...        \n",
      "2        1       0        68.00      0              0      0      ...        \n",
      "3        0       0        32.90      0              0      0      ...        \n",
      "4        0       0       100.00      0              0      0      ...        \n",
      "\n",
      "   revives  rideDistance  roadKills  swimDistance  teamKills  vehicleDestroys  \\\n",
      "0        0        0.0000          0          0.00          0                0   \n",
      "1        0        0.0045          0         11.04          0                0   \n",
      "2        0        0.0000          0          0.00          0                0   \n",
      "3        0        0.0000          0          0.00          0                0   \n",
      "4        0        0.0000          0          0.00          0                0   \n",
      "\n",
      "   walkDistance  weaponsAcquired  winPoints  winPlacePerc  \n",
      "0        244.80                1       1466        0.4444  \n",
      "1       1434.00                5          0        0.6400  \n",
      "2        161.80                2          0        0.7755  \n",
      "3        202.70                3          0        0.1667  \n",
      "4         49.75                2          0        0.1875  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "4446965\n"
     ]
    }
   ],
   "source": [
    "train_filepath = \"pubg-finish-placement-prediction/train_V2_clean.csv\"\n",
    "\n",
    "test_filepath = \"pubg-finish-placement-prediction/test_V2.csv\"\n",
    "\n",
    "#trainset_file = open(train_filepath,'rU')\n",
    "trainset = pd.read_csv(train_filepath)\n",
    "trainset = trainset.drop(['matchType'],axis=1)\n",
    "\n",
    "#testset_file = open(test_filepath,'rU')\n",
    "testset = pd.read_csv(test_filepath)\n",
    "\n",
    "print(trainset.head())\n",
    "testset.head()\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len old trainset 4446965\n",
      "len new trainset 3557572\n",
      "len valset 889393\n"
     ]
    }
   ],
   "source": [
    "#STEP 2\n",
    "\n",
    "\n",
    "#make baseline train and validation set --> 80:20 --> we can do kfold validation later \n",
    "print(\"len old trainset\",len(trainset))\n",
    "\n",
    "trainset_split = int(.8 * len(trainset))\n",
    "valset = trainset[trainset_split:]\n",
    "trainset = trainset[:trainset_split]\n",
    "\n",
    "print(\"len new trainset\",len(trainset))\n",
    "print(\"len valset\", len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'Id', 'groupId', 'matchId', 'assists', 'boosts',\n",
       "       'damageDealt', 'DBNOs', 'headshotKills', 'heals', 'killPlace',\n",
       "       'killPoints', 'kills', 'killStreaks', 'longestKill',\n",
       "       'matchDuration', 'maxPlace', 'numGroups', 'rankPoints', 'revives',\n",
       "       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n",
       "       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'winPoints',\n",
       "       'winPlacePerc'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_sorted = dict()\n",
    "\n",
    "#sort by matches \n",
    "trainset_sorted['matchId'] = trainset.groupby('matchId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of trainset 3557572\n",
      "num matches 47964\n",
      "(3557572, 29)\n"
     ]
    }
   ],
   "source": [
    "#check to make sure sorting is all good\n",
    "print(\"len of trainset\",len(trainset))\n",
    "print(\"num matches\",len(trainset_sorted['matchId']))\n",
    "print(trainset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket labels \n",
    "def bucket_labels(set_a,set_b,n_classes):\n",
    "    le = preprocessing.LabelEncoder() #100 classes\n",
    "\n",
    "    trainset_labels = le.fit_transform(set_a[outcome])\n",
    "    print(trainset_labels)\n",
    "    print(len(trainset_labels))\n",
    "    valset_labels = le.fit_transform(set_a[outcome])\n",
    "    labels = [trainset_labels,valset_labels]\n",
    "    result = []\n",
    "    for curr_labels in labels:\n",
    "        current_classes = max(curr_labels)\n",
    "        curr_result = []\n",
    "        for label in curr_labels:\n",
    "            n = ((1.0 * label)/current_classes)*n_classes\n",
    "            curr_result.append(round(n))\n",
    "        result.append(curr_result)\n",
    "    \n",
    "    print(min(result[0]))\n",
    "    return (result[0],result[1])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      kills   assists  damageDealt     heals  weaponsAcquired  winPoints  \\\n",
      "0 -0.592951 -0.397281    -0.764970 -0.511302        -1.081570   1.161743   \n",
      "1 -0.592951 -0.397281    -0.229705 -0.511302         0.543929  -0.820045   \n",
      "2 -0.592951  1.302284    -0.367047 -0.511302        -0.675195  -0.820045   \n",
      "3 -0.592951 -0.397281    -0.572445 -0.511302        -0.268820  -0.820045   \n",
      "4  0.048165 -0.397281    -0.179789 -0.511302        -0.675195  -0.820045   \n",
      "\n",
      "    assists  \n",
      "0 -0.397281  \n",
      "1 -0.397281  \n",
      "2  1.302284  \n",
      "3 -0.397281  \n",
      "4 -0.397281  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.1.1/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:15: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/Cellar/ipython/7.1.1/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "#start of fraction section\n",
    "#KNN\n",
    "\n",
    "outcome = 'winPlacePerc'\n",
    "\n",
    "features = ['kills','assists','damageDealt','heals','weaponsAcquired','winPoints','assists']\n",
    "neighbors = 15\n",
    "abbrev_trainset_labels = trainset[outcome]\n",
    "abbrev_valset_labels = valset[outcome]\n",
    "abbrev_trainset = trainset[features]\n",
    "abbrev_valset = valset[features]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(abbrev_trainset)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(abbrev_trainset)\n",
    "X_test = scaler.transform(abbrev_valset)  # apply same transformation to test data\n",
    "X_test = pd.DataFrame(X_test, columns=features)\n",
    "X_train = pd.DataFrame(X_train, columns=features)\n",
    "print(type(X_train))\n",
    "print(X_train.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trainset_labels,valset_labels = bucket_labels(abbrev_trainset,abbrev_valset,100)\n",
    "\n",
    "# predictions = dict()\n",
    "# classifier = None \n",
    "# for i in range(1,neighbors+1):\n",
    "#     print(i)\n",
    "#     classifier = KNeighborsClassifier(i)\n",
    "#     classifier.fit(abbrev_trainset[features], trainset_labels)\n",
    "#     predictions[i] = classifier.predict(abbrev_valset[features])\n",
    "\n",
    "\n",
    "# cm = confusion_matrix(valset_labels, multi_class_predict)\n",
    "# plt.figure(figsize = (100,70))\n",
    "# sn.heatmap(cm, annot=True)\n",
    "# sn.set(font_scale=1.4)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Truth\")\n",
    "\n",
    "#Step3 \n",
    "#SVM RBF kernel\n",
    "# svm = SVR().fit(abbrev_trainset[features],abbrev_trainset[outcome])\n",
    "# svm_predictions = svm.predict(abbrev_valset[features])\n",
    "# mea_svm_predictions = sklearn.metrics.mean_absolute_error(abbrev_valset[outcome], svm_predictions)\n",
    "# print(mea_svm_predictions)\n",
    "# cm_rbf = confusion_matrix(valset_labels, svm_predictions)\n",
    "# plt.figure(figsize = (100,70))\n",
    "# sn.heatmap(cm_rbf, annot=True)\n",
    "# sn.set(font_scale=1.4)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Truth\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.14, NNZs: 7, Bias: 0.471820, T: 3557572, Avg. loss: 0.026285\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.16, NNZs: 7, Bias: 0.470480, T: 7115144, Avg. loss: 0.026207\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.16, NNZs: 7, Bias: 0.474566, T: 10672716, Avg. loss: 0.026185\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.16, NNZs: 7, Bias: 0.473068, T: 14230288, Avg. loss: 0.026161\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.16, NNZs: 7, Bias: 0.475016, T: 17787860, Avg. loss: 0.026157\n",
      "Total training time: 3.75 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.17, NNZs: 7, Bias: 0.470724, T: 21345432, Avg. loss: 0.026152\n",
      "Total training time: 4.49 seconds.\n",
      "Convergence after 6 epochs took 4.49 seconds\n",
      "0.1804092407802669\n"
     ]
    }
   ],
   "source": [
    "#Step 2 \n",
    "#Linear Regression\n",
    "clf = linear_model.SGDRegressor(tol=1e-3, verbose=1)\n",
    "clf.fit(X_train,abbrev_trainset_labels)\n",
    "lr_predict = clf.predict(X_test)\n",
    "mae_lr = sklearn.metrics.mean_absolute_error(abbrev_valset_labels, lr_predict)\n",
    "# lr_train_predict = lr.predict(abbrev_trainset[features])\n",
    "# mea_train_lr = sklearn.metrics.mean_absolute_error(abbrev_trainset[outcome], lr_train_predict)\n",
    "print(mae_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n_classes = max(valset_labels)\n",
    "for i in range(1,neighbors):\n",
    "    print(i,sklearn.metrics.mean_absolute_error(valset_labels,predictions[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forest of trees\n",
    "features = ['year','age','genre1_num','gender_num']\n",
    "trees = 10\n",
    "rf = RandomForestClassifier(n_estimators=trees)\n",
    "rf.fit(train[features],train['rating'])\n",
    "predictions_FT = rf.predict(test[features])\n",
    "# Calculate accuracy\n",
    "numtrain = len(train)\n",
    "numtest = len(test)\n",
    "correct = 0\n",
    "for i in range(numtest):\n",
    "#    print 'Predicted:', predictions[i], ' Actual:', citiesTest.loc[numtrain+i]['category']\n",
    "#    if predictions_FT[i] == test.loc[numtrain+i]['rating']: correct +=1\n",
    "#print 'Accuracy:', float(correct)/float(numtest)\n",
    "       \n",
    "       \n",
    "    print 'Predicted:', predictions_FT[i], ' Actual:', train.iloc[i]['rating']\n",
    "    if predictions_FT[i] == test.iloc[i]['rating']: correct +=1\n",
    "print 'Accuracy:', float(correct)/float(numtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes\n",
    "features = ['year','age','genre1_num','gender_num']\n",
    "nb = GaussianNB()\n",
    "nb.fit(train[features],train['rating'])\n",
    "predictions_NB = nb.predict(test[features])\n",
    "# Calculate accuracy\n",
    "numtrain = len(train)\n",
    "numtest = len(test)\n",
    "correct = 0\n",
    "for i in range(numtest):\n",
    "    print('Predicted:', predictions_NB[i], ' Actual:', train.iloc[i]['rating'])\n",
    "    if predictions_NB[i] == test.iloc[i]['rating']: correct +=1\n",
    "print('Accuracy:', float(correct)/float(numtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find mean\n",
    "avgpredicts = []\n",
    "for i in range(201):\n",
    "    avgpredict=1.0*(predictions_KNN[i]+predictions_FT[i]+predictions_NB[i])/2\n",
    "    avgpredicts.append(avgpredict)\n",
    "for i, avgpredict in enumerate(avgpredicts):\n",
    "    if avgpredict > 5: \n",
    "        avgpredicts[i] = 5.0\n",
    "print(avgpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3\n",
    "\n",
    "\n",
    "#Exports real numbers predictions to csv - named V1predict.csv\n",
    "import csv\n",
    "input_file = csv.reader(open(\"predict.csv\"))\n",
    "# This is the file where you will save the result.\n",
    "ofile = open('V1predict.csv', \"w\")\n",
    "# This is a way to use csv.writer.\n",
    "writer = csv.writer(ofile, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "# loop over each row of predict.csv\n",
    "for row_num, row in enumerate(input_file):\n",
    "    # if header row, just write it and does nothing else\n",
    "    if row_num == 0:\n",
    "        writer.writerow(row)\n",
    "        continue\n",
    "    # if not header row, put your own prediction for the second column of the current row\n",
    "    row[2] = str(avgpredicts[row_num-1])\n",
    "    # write out the row to the file\n",
    "    writer.writerow(row)\n",
    "# finish writing to the file and close it\n",
    "ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3\n",
    "\n",
    "\n",
    "#Exports integers predictions to csv - named V2predict.csv\n",
    "\n",
    "import csv\n",
    "input_file = csv.reader(open(\"predict.csv\"))\n",
    "# This is the file where you will save the result.\n",
    "ofile = open('V2predict.csv', \"w\")\n",
    "# This is a way to use csv.writer.\n",
    "writer = csv.writer(ofile, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "# loop over each row of predict.csv\n",
    "for row_num, row in enumerate(input_file):\n",
    "    # if header row, just write it and does nothing else\n",
    "    if row_num == 0:\n",
    "        writer.writerow(row)\n",
    "        continue\n",
    "    # if not header row, put your own prediction for the second column of the current row\n",
    "    row[2] = str(predictions[row_num-1])\n",
    "    # write out the row to the file\n",
    "    writer.writerow(row)\n",
    "# finish writing to the file and close it\n",
    "ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
