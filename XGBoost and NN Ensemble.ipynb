{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For compatibility across multiple platforms\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "# Load files using DictReader in Python\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from ultimate.mlp import MLP \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"pubg-finish-placement-prediction/train_V2_clean.csv\"\n",
    "\n",
    "#trainset_file = open(train_filepath,'rU')\n",
    "trainset = pd.read_csv(train_filepath,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4446965\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_two(moddedTrain):\n",
    "    moddedTrain['playersJoined'] = moddedTrain.groupby('matchId')['matchId'].transform('count')\n",
    "    moddedTrain['killsNorm'] = moddedTrain['kills']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['damageDealtNorm'] = moddedTrain['damageDealt']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['maxPlaceNorm'] = moddedTrain['maxPlace']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['matchDurationNorm'] = moddedTrain['matchDuration']*((100-moddedTrain['playersJoined'])/100 + 1)\n",
    "    moddedTrain['healsandboosts'] = moddedTrain['heals'] + moddedTrain['boosts']\n",
    "    moddedTrain['totalDistance'] = moddedTrain['rideDistance'] + moddedTrain['walkDistance'] + moddedTrain['swimDistance']\n",
    "    moddedTrain['killsWithoutMoving'] = ((moddedTrain['kills'] > 0) & (moddedTrain['totalDistance'] == 0))\n",
    "    moddedTrain['headshot_rate'] = moddedTrain['headshotKills'] / moddedTrain['kills']\n",
    "    moddedTrain['headshot_rate'] = moddedTrain['headshot_rate'].fillna(0)\n",
    "    moddedTrain.drop(moddedTrain[moddedTrain['killsWithoutMoving'] == True].index, inplace=True)\n",
    "    moddedTrain.drop(moddedTrain[moddedTrain['roadKills'] > 8].index, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(is_train=True):\n",
    "    # When this function is used for the training data, load train_V2.csv :\n",
    "    if is_train: \n",
    "        print(\"processing train_V2.csv\")\n",
    "        df = pd.read_csv(\"pubg-finish-placement-prediction/train_V2_clean.csv\",index_col=0)\n",
    "        \n",
    "        # Only take the samples with matches that have more than 1 player \n",
    "        # there are matches with no players or just one player ( those samples could affect our model badly) \n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    \n",
    "    # When this function is used for the test data, load test_V2.csv :\n",
    "    else:\n",
    "        print(\"processing test_V2.csv\")\n",
    "        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "        \n",
    "    # Make a new feature indecating the total distance a player cut :\n",
    " \n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "\n",
    "    # Process the 'rankPoints' feature by replacing any value of (-1) to be (0) :\n",
    "    df['rankPoints'] = np.where(df['rankPoints'] <= 0 ,0 , df['rankPoints'])\n",
    "                           \n",
    "    \n",
    "\n",
    "    target = 'winPlacePerc'\n",
    "    # Get a list of the features to be used\n",
    "    features = list(df.columns)\n",
    "    \n",
    "    # Remove some features from the features list :\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchDuration\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    y = None\n",
    "    \n",
    "    # If we are processing the training data, process the target\n",
    "    # (group the data by the match and the group then take the mean of the target) \n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        # Remove the target from the features list :\n",
    "        features.remove(target)\n",
    "    \n",
    "    # Make new features indicating the mean of the features ( grouped by match and group ) :\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    \n",
    "    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n",
    "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
    "    # If we are processing the test data let df_out = 'matchId' and 'groupId' without grouping \n",
    "    else: df_out = df[['matchId','groupId']]\n",
    "    \n",
    "    # Merge agg and agg_rank (that we got before) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the max value of the features for each group ( grouped by match )\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    # Put the new features into a rank form ( max value will have the highest rank)\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the number of players in each group ( grouped by match )\n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "     \n",
    "    # Merge the group_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the mean value of each features for each match :\n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    \n",
    "    # Merge the new agg with df_out :\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    # Make new features indicating the number of groups in each match :\n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    \n",
    "    # Merge the match_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    # Drop matchId and groupId\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "    \n",
    "    y = y.tolist()\n",
    "    \n",
    "\n",
    "    return df_out,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing train_V2.csv\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n"
     ]
    }
   ],
   "source": [
    "df_out,y = feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_out, y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# SET UP FOR MODEL ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the trainset features for tree one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = x_train\n",
    "y_one = y_train\n",
    "\n",
    "x_one_val = x_val\n",
    "y_one_val = y_val\n",
    "\n",
    "\n",
    "#set the dmatrix\n",
    "#trainset_one_dmatrix = xgb.DMatrix(x_one.values,label=y_train.values,feature_names=x_one.columns)\n",
    "#valset_one_dmatrix = xgb.DMatrix(x_one_val.values,label=y_one_val.values,feature_names=x_one_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_one_dmatrix = xgb.DMatrix(x_one.values,label=y_train,feature_names=x_one.columns)\n",
    "valset_one_dmatrix = xgb.DMatrix(x_one_val.values,label=y_one_val,feature_names=x_one_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assists_mean</th>\n",
       "      <th>boosts_mean</th>\n",
       "      <th>damageDealt_mean</th>\n",
       "      <th>DBNOs_mean</th>\n",
       "      <th>headshotKills_mean</th>\n",
       "      <th>heals_mean</th>\n",
       "      <th>killPlace_mean</th>\n",
       "      <th>killPoints_mean</th>\n",
       "      <th>kills_mean</th>\n",
       "      <th>killStreaks_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>rideDistance</th>\n",
       "      <th>roadKills</th>\n",
       "      <th>swimDistance</th>\n",
       "      <th>teamKills</th>\n",
       "      <th>vehicleDestroys</th>\n",
       "      <th>walkDistance</th>\n",
       "      <th>weaponsAcquired</th>\n",
       "      <th>winPoints</th>\n",
       "      <th>totalDistance</th>\n",
       "      <th>match_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1008717</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1227.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125.435714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.772255</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.093939</td>\n",
       "      <td>3.214286</td>\n",
       "      <td>1498.918367</td>\n",
       "      <td>1166.301908</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672723</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>254.066667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>219.021213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.457010</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1257.642292</td>\n",
       "      <td>3.427083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1482.120515</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940897</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>138.420000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1234.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1196.504167</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>2.019740</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>1187.176000</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>1513.843750</td>\n",
       "      <td>2385.699906</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825965</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.090638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.266957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>1231.427117</td>\n",
       "      <td>3.765957</td>\n",
       "      <td>1519.372340</td>\n",
       "      <td>1281.784713</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338466</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>171.066667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>20.755914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014785</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>963.941043</td>\n",
       "      <td>3.139785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>984.711742</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         assists_mean  boosts_mean  damageDealt_mean  DBNOs_mean  \\\n",
       "1008717      0.000000     0.000000         33.333333    0.333333   \n",
       "672723       0.333333     0.666667        254.066667    2.000000   \n",
       "1940897      0.000000     2.000000        138.420000    0.500000   \n",
       "825965       0.000000     0.000000          0.000000    0.000000   \n",
       "338466       0.000000     0.000000        171.066667    1.666667   \n",
       "\n",
       "         headshotKills_mean  heals_mean  killPlace_mean  killPoints_mean  \\\n",
       "1008717            0.000000    0.000000       92.000000      1227.666667   \n",
       "672723             1.000000    1.000000       20.333333         0.000000   \n",
       "1940897            0.500000    4.000000       31.000000      1234.000000   \n",
       "825965             0.000000    0.000000       57.000000      1000.000000   \n",
       "338466             0.333333    0.333333       44.000000         0.000000   \n",
       "\n",
       "         kills_mean  killStreaks_mean     ...      rideDistance  roadKills  \\\n",
       "1008717    0.000000          0.000000     ...        125.435714   0.000000   \n",
       "672723     2.333333          1.666667     ...        219.021213   0.000000   \n",
       "1940897    1.000000          0.500000     ...       1196.504167   0.020833   \n",
       "825965     0.000000          0.000000     ...         43.090638   0.000000   \n",
       "338466     0.666667          0.666667     ...         20.755914   0.000000   \n",
       "\n",
       "         swimDistance  teamKills  vehicleDestroys  walkDistance  \\\n",
       "1008717      7.772255   0.010204         0.000000   1033.093939   \n",
       "672723       5.457010   0.020833         0.000000   1257.642292   \n",
       "1940897      2.019740   0.010417         0.020833   1187.176000   \n",
       "825965       7.266957   0.000000         0.010638   1231.427117   \n",
       "338466       0.014785   0.032258         0.000000    963.941043   \n",
       "\n",
       "         weaponsAcquired    winPoints  totalDistance  match_size  \n",
       "1008717         3.214286  1498.918367    1166.301908          98  \n",
       "672723          3.427083     0.000000    1482.120515          96  \n",
       "1940897         4.291667  1513.843750    2385.699906          96  \n",
       "825965          3.765957  1519.372340    1281.784713          94  \n",
       "338466          3.139785     0.000000     984.711742          93  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure tree one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\" : 7,\n",
    "    \"eval_metric\" : [\"mae\"],\n",
    "    \"lambda\": 1.1,   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:20:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[0]\ttrain-mae:0.184913\tval-mae:0.184922\n",
      "[23:21:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[1]\ttrain-mae:0.134009\tval-mae:0.134005\n",
      "[23:22:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[2]\ttrain-mae:0.09982\tval-mae:0.099816\n",
      "[23:23:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[3]\ttrain-mae:0.077286\tval-mae:0.077257\n",
      "[23:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[4]\ttrain-mae:0.062522\tval-mae:0.062493\n",
      "[23:25:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[5]\ttrain-mae:0.053126\tval-mae:0.053112\n",
      "[23:26:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[6]\ttrain-mae:0.04652\tval-mae:0.046509\n",
      "[23:27:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[7]\ttrain-mae:0.042367\tval-mae:0.042366\n",
      "[23:28:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[8]\ttrain-mae:0.039947\tval-mae:0.039957\n",
      "[23:29:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[9]\ttrain-mae:0.038201\tval-mae:0.038218\n",
      "[23:30:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[10]\ttrain-mae:0.037137\tval-mae:0.037174\n",
      "[23:31:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[11]\ttrain-mae:0.036134\tval-mae:0.036177\n",
      "[23:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12]\ttrain-mae:0.035617\tval-mae:0.035683\n",
      "[23:32:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[13]\ttrain-mae:0.035208\tval-mae:0.035289\n",
      "[23:33:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[14]\ttrain-mae:0.034926\tval-mae:0.035024\n",
      "[23:34:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[15]\ttrain-mae:0.03468\tval-mae:0.034795\n",
      "[23:35:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[16]\ttrain-mae:0.034357\tval-mae:0.034474\n",
      "[23:36:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[17]\ttrain-mae:0.034129\tval-mae:0.034259\n",
      "[23:37:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[18]\ttrain-mae:0.033981\tval-mae:0.034118\n",
      "[23:38:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[19]\ttrain-mae:0.033802\tval-mae:0.033954\n"
     ]
    }
   ],
   "source": [
    "iters = 20\n",
    "\n",
    "tree_one = xgb.train(params, trainset_one_dmatrix, evals=[(trainset_one_dmatrix, \"train\"),(valset_one_dmatrix, 'val')], num_boost_round = iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb.plot_importance(tree_one,importance_type='cover')\n",
    "xgb.plot_importance(tree_one,importance_type='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP FOR MODEL TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the trainset features for tree two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_two = x_train\n",
    "y_two = y_train\n",
    "\n",
    "x_two_val = x_val\n",
    "y_two_val = y_val\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -1.        , -0.98992342, ...,  0.920351  ,\n",
       "        -0.69493471,  0.95789474],\n",
       "       [-0.96825397, -0.95959596, -0.92319629, ..., -1.        ,\n",
       "        -0.6123272 ,  0.91578947],\n",
       "       [-1.        , -0.87878788, -0.95815599, ...,  0.93947277,\n",
       "        -0.37598127,  0.91578947],\n",
       "       ...,\n",
       "       [-1.        , -0.96969697, -0.98488513, ..., -1.        ,\n",
       "        -0.74718103,  0.87368421],\n",
       "       [-0.97619048, -1.        , -0.97965538, ...,  0.93687894,\n",
       "        -0.71874774,  0.87368421],\n",
       "       [-1.        , -0.90909091, -0.98606409, ...,  0.93673303,\n",
       "        -0.43835967,  0.89473684]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(x_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 136)               23256     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               13700     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 122,023\n",
      "Trainable params: 122,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create NN_model\n",
    "NN_model = Sequential()\n",
    "NN_model.add(Dense(x_two.shape[1],  input_dim = x_two.shape[1], activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(136, activation='relu'))\n",
    "NN_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output Layer\n",
    "NN_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1297116 samples, validate on 324279 samples\n",
      "Epoch 1/10\n",
      "1297116/1297116 [==============================] - 35s 27us/step - loss: 0.0570 - mean_absolute_error: 0.0570 - val_loss: 0.0546 - val_mean_absolute_error: 0.0546\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.06112 to 0.05457, saving model to Weights-001--0.05457.hdf5\n",
      "Epoch 2/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0545 - mean_absolute_error: 0.0545 - val_loss: 0.0513 - val_mean_absolute_error: 0.0513\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05457 to 0.05129, saving model to Weights-002--0.05129.hdf5\n",
      "Epoch 3/10\n",
      "1297116/1297116 [==============================] - 25s 20us/step - loss: 0.0527 - mean_absolute_error: 0.0527 - val_loss: 0.0538 - val_mean_absolute_error: 0.0538\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.05129\n",
      "Epoch 4/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0613 - val_mean_absolute_error: 0.0613\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.05129\n",
      "Epoch 5/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0503 - mean_absolute_error: 0.0503 - val_loss: 0.0521 - val_mean_absolute_error: 0.0521\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.05129\n",
      "Epoch 6/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0488 - mean_absolute_error: 0.0488 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05129 to 0.04657, saving model to Weights-006--0.04657.hdf5\n",
      "Epoch 7/10\n",
      "1297116/1297116 [==============================] - 24s 19us/step - loss: 0.0482 - mean_absolute_error: 0.0482 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04657\n",
      "Epoch 8/10\n",
      "1297116/1297116 [==============================] - 27s 21us/step - loss: 0.0477 - mean_absolute_error: 0.0477 - val_loss: 0.0482 - val_mean_absolute_error: 0.0482\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04657\n",
      "Epoch 9/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0468 - mean_absolute_error: 0.0468 - val_loss: 0.0545 - val_mean_absolute_error: 0.0545\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04657\n",
      "Epoch 10/10\n",
      "1297116/1297116 [==============================] - 25s 19us/step - loss: 0.0466 - mean_absolute_error: 0.0466 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a61e13dd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(x=x_two, y=y_two, batch_size=1000,\n",
    "             epochs=10, verbose=1, callbacks=callbacks_list,\n",
    "             validation_split=0.20, validation_data=None, shuffle=True,\n",
    "             class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "             steps_per_epoch=None, validation_steps=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSENBLE MODELS ON VALSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict - Update - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_predictions(models,labels,num_samples):\n",
    "    labels = labels[:num_samples]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        data = model['data']\n",
    "        predictor = model['predictor']\n",
    "\n",
    "        if model['type'] == \"tree\":\n",
    "            matrix = xgb.DMatrix(data,feature_names=data.columns)\n",
    "            prediction = predictor.predict(matrix)\n",
    "        if model['type'] == \"nn\":\n",
    "            prediction = predictor.predict(data)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    avg_predictions = [0.0]*len(predictions[0])\n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        for j in range(len(predictions[0])):\n",
    "            avg_predictions[j] += 1.0* predictions[i][j] / len(predictions)\n",
    "\n",
    "    print(len(avg_predictions))\n",
    "    return avg_predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405349\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append({'data':x_one_val,\n",
    "                'predictor':NN_model,\n",
    "                'type':\"nn\"})\n",
    "\n",
    "models.append({'data':x_one_val,\n",
    "                'predictor':tree_one,\n",
    "                'type':\"tree\"})\n",
    "\n",
    "\n",
    "\n",
    "labels = y_val\n",
    "\n",
    "num_samples = 200000\n",
    "\n",
    "predictions = gen_predictions(models,labels,num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03395361648337445\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.mean_absolute_error(labels,tree_one.predict(valset_one_dmatrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
